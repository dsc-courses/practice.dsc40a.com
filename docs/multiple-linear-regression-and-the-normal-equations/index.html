<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Multiple Linear Regression and The Normal Equations</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="..\assets\theme.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Multiple Linear Regression and The Normal
Equations</h1>
</header>
<p><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
<!-- add after bootstrap.min.css -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<!-- add after bootstrap.min.js or bootstrap.bundle.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script></p>
<!-- for difficulty gauges-->
<script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B947E6J6H4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-B947E6J6H4');
</script>
<p><a href="../index.html">← return to practice.dsc40a.com</a></p>
<hr />
<p>This page contains all problems about Multiple Linear Regression and
The Normal Equations.</p>
<hr />
<h2 id="problem-1">Problem 1</h2>
<p><i>Source: <a href="../sp23-midterm1/index.html">Spring 2023 Midterm
1</a>, Problem 5</i></p>
<p>Let <span class="math inline">X</span> be a design matrix with 4
columns, such that the first column is a column of all <span
class="math inline">1</span>s. Let <span
class="math inline">\vec{y}</span> be an observation vector. Let <span
class="math inline">\vec{w}^* = (X^TX)^{-1}X^T\vec{y}.</span> We’ll name
the components of <span class="math inline">\vec{w}^*</span> as
follows:</p>
<p><span class="math display">\vec{w}^* = \begin{bmatrix} w_0^* \\ w_1^*
\\ w_2^* \\ w_3^* \end{bmatrix}</span></p>
<p>In this problem, we’ll consider various modifications to the design
matrix and see how they affect the solution to the normal equations.</p>
<p><br></p>
<h3 id="problem-1.1">Problem 1.1</h3>
<p>Let <span class="math inline">X_a</span> be the design matrix that
comes from <strong>interchanging the first two columns</strong> of <span
class="math inline">X</span>. Let <span class="math inline">\vec{w_a}^*
= (X_a^TX_a)^{-1}X_a^T\vec{y}</span>. Express the components <span
class="math inline">\vec{w_a}^*</span> in terms of <span
class="math inline">w_0^*, w_1^*, w_2^*</span>, and <span
class="math inline">w_3^*</span> (which were the components of <span
class="math inline">\vec{w}^*</span>).</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading1_1">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse1_1" aria-expanded="true" aria-controls="collapse1_1">
Click to view the solution.
</button>
</h2>
<div id="collapse1_1" class="accordion-collapse collapse"
aria-labelledby="heading1_1" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math display">\vec{w_a}^* = \begin{bmatrix} w_1^* \\
w_0^* \\ w_2^* \\ w_3^* \end{bmatrix}</span></p>
<p>Suppose our original prediction rule was of the form: <span class="math display">H(\vec{x}) = w_0 + w_1x_1+
w_2x_2+  w_3x_3.</span></p>
<p>Where: <span class="math display">\vec{w_a}^* = \begin{bmatrix} v_0^*
\\ v_1^* \\ v_2^* \\ v_3^* \end{bmatrix}</span></p>
<p>By swapping the first two columns of our design matrix, this changes
the prediction rule to be of the form: <span class="math display">H_2(\vec{x}) = v_1 + v_0x_1 +
v_2x_2+  v_3x_3.</span></p>
<p>Therefore the optimal parameters for <span class="math inline">H_2</span> are related to the optimal parameters for
<span class="math inline">H</span> by: <span class="math display">\begin{aligned} v_0^* &amp;= w_1^* \\ v_1^* &amp;=
w_0^* \\ v_2^* &amp;= w_2^* \\ v_3^* &amp;= w_3^*
\end{aligned}</span></p>
<p>Intuitively, when we interchange two columns of our design matrix,
all that does is interchange the terms in the prediction rule, which
interchanges those weights in the parameter vector.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<h3 id="problem-1.2">Problem 1.2</h3>
<p>Let <span class="math inline">X_b</span> be the design matrix that
comes from <strong>adding one to each entry of the first column</strong>
of <span class="math inline">X</span>. Let <span
class="math inline">\vec{w_b}^* = (X_b^TX_b)^{-1}X_b^T\vec{y}</span>.
Express the components <span class="math inline">\vec{w_b}^*</span> in
terms of <span class="math inline">w_0^*, w_1^*, w_2^*</span>, and <span
class="math inline">w_3^*</span> (which were the components of <span
class="math inline">\vec{w}^*</span>).</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading1_2">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse1_2" aria-expanded="true" aria-controls="collapse1_2">
Click to view the solution.
</button>
</h2>
<div id="collapse1_2" class="accordion-collapse collapse"
aria-labelledby="heading1_2" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math display">\vec{w_b}^* = \begin{bmatrix}
\dfrac{w_0^*}{2}  \\ w_1^* \\ w_2^* \\ w_3^*\end{bmatrix}</span></p>
<p>Suppose our original prediction rule was of the form: <span class="math display">H(\vec{x}) = w_0 + w_1x_1+
w_2x_2+  w_3x_3.</span></p>
<p>Where: <span class="math display">\vec{w_b}^* = \begin{bmatrix} v_0^*
\\ v_1^* \\ v_2^* \\ v_3^* \end{bmatrix}</span></p>
<p>By adding one to each entry of the first column of the design matrix,
we are changing the column of <span class="math inline">1</span>s to be
a column of <span class="math inline">2</span>s. This changes the
prediction rule to be of the form: <span class="math display">H_2(\vec{x}) = v_0\cdot 2+ v_1x_1 +
v_2x_2+  v_3x_3.</span></p>
<p>In order to compensate for these changes to our coefficients, we need
to “offset” any alterations made to our coefficients. Therefore the
optimal parameters for <span class="math inline">H_2</span> are related
to the optimal parameters for <span class="math inline">H</span> by:
<span class="math display">\begin{aligned} v_0^* &amp;= \dfrac{w_0^*}{2}
\\ v_1^* &amp;= w_1^* \\ v_2^* &amp;= w_2^* \\ v_3^* &amp;= w_3^*
\end{aligned}</span></p>
<p>This is saying we just halve the intercept term. For example, imagine
fitting a line to data in <span class="math inline">\mathbb{R}^2</span>
and finding that the best-fitting line is <span class="math inline">y=12+3x</span>. If we had to write this in the form
<span class="math inline">y=v_0\cdot 2 + v_1x</span>, we would find that
the best choice for <span class="math inline">v_0</span> is <span class="math inline">6</span> and the best choice for <span class="math inline">v_1</span> is <span class="math inline">3</span>.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<h3 id="problem-1.3">Problem 1.3</h3>
<p>Let <span class="math inline">X_c</span> be the design matrix that
comes from <strong>adding one to each entry of the third column</strong>
of <span class="math inline">X</span>. Let <span
class="math inline">\vec{w_c}^* = (X_c^TX_c)^{-1}X_c^T\vec{y}</span>.
Express the components <span class="math inline">\vec{w_c}^*</span> in
terms of <span class="math inline">w_0^*, w_1^*, w_2^*</span>, and <span
class="math inline">w_3^*</span>, which were the components of <span
class="math inline">\vec{w}^*</span>.</p>
<p><span class="math display">\vec{w_c}^* = \begin{bmatrix} w_0^* -
w_2^*  \\ w_1^* \\ w_2^* \\ w_3^* \end{bmatrix}</span></p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading1_3">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse1_3" aria-expanded="true" aria-controls="collapse1_3">
Click to view the solution.
</button>
</h2>
<div id="collapse1_3" class="accordion-collapse collapse"
aria-labelledby="heading1_3" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p>Suppose our original prediction rule was of the form: <span class="math display">H(\vec{x}) = w_0 + w_1x_1+
w_2x_2+  w_3x_3.</span></p>
<p>Where: <span class="math display">\vec{w_c}^* = \begin{bmatrix} v_0^*
\\ v_1^* \\ v_2^* \\ v_3^* \end{bmatrix}</span></p>
<p>By adding one to each entry of the third column of the design matrix,
this changes the prediction rule to be of the form: <span class="math display">\begin{aligned} H_2(\vec{x}) &amp;= v_0+ v_1x_1 +
v_2(x_2+1)+  v_3x_3 \\ &amp;= (v_0 + v_2) + v_1x_1 + v_2x_2+  v_3x_3
\end{aligned}</span></p>
<p>In order to compensate for these changes to our coefficients, we need
to “offset” any alterations made to our coefficients. Therefore the
optimal parameters for <span class="math inline">H_2</span> are related
to the optimal parameters for <span class="math inline">H</span> by
<span class="math display">\begin{aligned} v_0^* &amp;= w_0^* - w_2^* \\
v_1^* &amp;= w_1^* \\ v_2^* &amp;= w_2^* \\ v_3^* &amp;= w_3^*
\end{aligned}</span></p>
<p>One way to think about this is that if we replace <span class="math inline">x_2</span> with <span class="math inline">x_2+1</span>, then our predictions will increase by
the coefficient of <span class="math inline">x_2</span>. In order to
keep our predictions the same, we would need to adjust our intercept
term by subtracting this same amount.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<hr />
<h2 id="problem-2">Problem 2</h2>
<p><i>Source: <a href="../sp23-final-pt1/index.html">Spring 2023 Final
Part 1</a>, Problem 5</i></p>
<p>Suppose we want to predict how long it takes to run a Jupyter
notebook on Datahub. For 100 different Jupyter notebooks, we collect the
following 5 pieces of information:</p>
<ul>
<li><p><strong>cells</strong>: number of cells in the notebook</p></li>
<li><p><strong>lines</strong>: number of lines of code</p></li>
<li><p><strong>max iterations</strong>: largest number of iterations in
any loop in the notebook, or 1 if there are no loops</p></li>
<li><p><strong>variables</strong>: number of variables defined in the
notebook</p></li>
<li><p><strong>runtime</strong>: number of seconds for the notebook to
run on Datahub</p></li>
</ul>
<p>Then we use multiple regression to fit a prediction rule of the form
<span class="math display">H(\text{cells, lines, max iterations,
variables}) =  w_0 + w_1 \cdot \text{cells} \cdot \text{lines} + w_2
\cdot (\text{max iterations})^{\text{variables} - 10}</span></p>
<p><br></p>
<h3 id="problem-2.1">Problem 2.1</h3>
<p>What are the dimensions of the design matrix <span
class="math inline">X</span>?</p>
<p><span class="math display">\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
\end{bmatrix}_{r \times c}</span></p>
<p>So, what should <span class="math inline">r</span> and <span
class="math inline">c</span> be for: <span class="math inline">r</span>
rows <span class="math inline">\times</span> <span
class="math inline">c</span> columns.</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading2_1">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse2_1" aria-expanded="true" aria-controls="collapse2_1">
Click to view the solution.
</button>
</h2>
<div id="collapse2_1" class="accordion-collapse collapse"
aria-labelledby="heading2_1" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math inline">100 \text{ rows} \times 3 \text{
columns}</span></p>
<p>There should be <span class="math inline">100</span> rows because
there are <span class="math inline">100</span> different Jupyter
notebooks with different information within them. There should be <span class="math inline">3</span> columns, one for each <span class="math inline">w_i</span>. In this case we have <span class="math inline">w_0</span>, which means <span class="math inline">X</span> will have a column of ones, <span class="math inline">w_1</span>, which means <span class="math inline">X</span> will have a second column of <span class="math inline">\text{cells} \cdot \text{lines}</span>, and <span class="math inline">w_2</span>, which will be the last column in <span class="math inline">X</span> containing <span class="math inline">\text{max iterations})^{\text{variables} -
10}</span>.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-2.2">Problem 2.2</h3>
<p>In <strong>one sentence</strong>, what does the entry in row 3,
column 2 of the design matrix X represent? (Count rows and columns
starting at 1, not 0).</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading2_2">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse2_2" aria-expanded="true" aria-controls="collapse2_2">
Click to view the solution.
</button>
</h2>
<div id="collapse2_2" class="accordion-collapse collapse"
aria-labelledby="heading2_2" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p>This entry represents the product of the number of cells and number
of lines of code for the third Jupyter notebook in the training
dataset.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<hr />
<h2 id="problem-3">Problem 3</h2>
<p><i>Source: <a href="../sp23-final-pt1/index.html">Spring 2023 Final
Part 1</a>, Problem 6</i></p>
Now we solve the normal equations and find the solution to be
<span class="math display">\begin{aligned}
            \vec{w}^* &amp;= \begin{bmatrix} w_0^* \\ w_1^* \\ w_2^*
\end{bmatrix}
\end{aligned}</span>
Define a new vector:
<span class="math display">\begin{aligned}
            \vec{w}^{\circ} &amp;= \begin{bmatrix} w_0^{\circ} \\
w_1^{\circ} \\ w_2^{\circ} \end{bmatrix} = \begin{bmatrix} w_0^*+3 \\
w_1^*-4 \\ w_2^*-6 \end{bmatrix}
            
\end{aligned}</span>
<p>and consider the two prediction rules</p>
<span class="math display">\begin{aligned}
        H^*(\text{cells, lines, max iterations, variables})
&amp;=  w_0^* + w_1^* \cdot \text{cells} \cdot \text{lines} + w_2^*
\cdot (\text{max iterations})^{\text{variables} - 10}\\
        H^{\circ}(\text{cells, lines, max iterations, variables})
&amp;=  w_0^{\circ} + w_1^{\circ} \cdot \text{cells} \cdot \text{lines}
+ w_2^{\circ} \cdot (\text{max iterations})^{\text{variables} - 10}
        
\end{aligned}</span>
<p>Let <span class="math inline">\text{MSE}</span> represent the mean
squared error of a prediction rule, and let <span
class="math inline">\text{MAE}</span> represent the mean absolute error
of a prediction rule. Select the symbol that should go in each
blank.</p>
<p><br></p>
<h3 id="problem-3.1">Problem 3.1</h3>
<p><span class="math inline">\text{MSE}(H^*)</span> ___ <span
class="math inline">\text{MSE}(H^{\circ})</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">\leq</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">\geq</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">=</span></p></li>
</ul>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading3_1">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse3_1" aria-expanded="true" aria-controls="collapse3_1">
Click to view the solution.
</button>
</h2>
<div id="collapse3_1" class="accordion-collapse collapse"
aria-labelledby="heading3_1" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math inline">\leq</span></p>
<p>It is given that <span class="math inline">\vec{w}^*</span> is the
optimal parameter vector. Here’s one thing we know about the optimal
parameter vector <span class="math inline">\vec{w}^*</span>: it is
optimal, which means that any <em>changes</em> made to it will, at best,
keep our predictions of the exact same quality, and, at worst, reduce
the quality of our predictions and <strong>increase our error</strong>.
And since <span class="math inline">\vec{w} \degree</span> is just the
optimal parameter vector but with some small <em>changes</em> to the
weights, it stands that <span class="math inline">\vec{w} \degree</span>
is liable to create equal or greater error!</p>
<p>In other words, <span class="math inline">\vec{w} \degree</span> is a
slightly worse version of <span class="math inline">\vec{w}^*</span>,
meaning that <span class="math inline">H \degree(x)</span> is a slightly
worse version of <span class="math inline">H^*(x)</span>. So, <span class="math inline">H \degree(x)</span> will have equal or higher error
than <span class="math inline">H^*(x)</span>.</p>
<p>Hence: <span class="math inline">\text{MSE}(H^*) \leq
\text{MSE}(H^{\circ})</span></p>
<!-- Recall the equation for mean squared error: $\text{MSE}(H(x_i)) = \frac{1}{n}\sum_{i=1}^n(y_i-H(x_i))^2$. We can figure out which is bigger by subtracting  $\text{MSE}(H^{\circ})$ from $\text{MSE}(H^*)$. The difference between the squared differences is:
$$(y_i-H^*(x_i))^2-(y_i-H^{\circ}(x_i))^2$$Notice there is a squared element, which means that the differences of $w^*_0 - w^{\circ}_0$, $w^*_1 - w^{\circ}_1$, and $w^*_2 - w^{\circ}_2$ will appear as squared terms, which are always positive! This means adding these squared differences to $H^*$ will make it at least as large as the squared difference for $H^{\circ}$. -->
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-3.2">Problem 3.2</h3>
<p><span class="math inline">(\text{MAE}(H^{\circ}))^2</span> ___ <span
class="math inline">\text{MSE}(H^{\circ})</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">\leq</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">\geq</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">=</span></p></li>
</ul>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading3_2">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse3_2" aria-expanded="true" aria-controls="collapse3_2">
Click to view the solution.
</button>
</h2>
<div id="collapse3_2" class="accordion-collapse collapse"
aria-labelledby="heading3_2" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math inline">\geq</span></p>
<p>The formula for mean absolute error is: <span class="math inline">\text{MAE}(H(x_i)) =
\frac{1}{n}\sum_{i=1}^n|y_i-H(x_i)|</span><br/>
The formula for mean squared error is: <span class="math inline">\text{MSE}(H(x_i)) =
\frac{1}{n}\sum_{i=1}^n(y_i-H(x_i))^2</span></p>
<p>We can substitute these two formulas into our comparison and recieve:
<span class="math inline">(\frac{1}{n}\sum_{i=1}^n|y_i-H(x_i)|)^2</span>
___ <span class="math inline">\frac{1}{n}\sum_{i=1}^n(y_i-H(x_i))^2</span></p>
<p>It is now clear that our comparison boils down to this question:
“Which is greater? If I take the absolute errors, square them, and then
sum them up… <em>or</em> if I take the absolute errors, sum them up, and
then square them?”</p>
<p>Rules from algebra tell us that summing the absolute errors up
<em>before</em> squaring them will always produce a greater result than
the other way around (this holds when all of our values are positive,
which they are in this case thanks to dealing with <em>absolute</em>
errors)! It’s a good exercise to think about why this is the case, but a
basic example is if I have two positive values <span class="math inline">a</span> and <span class="math inline">b</span>:
<span class="math inline">(a + b)^2 = a^2 + b^2 + 2ab \geq a^2 +
b^2</span>, since <span class="math inline">2ab</span> is a non-negative
term.</p>
<p>Bringing this altogether, we should conclude that <span class="math inline">\text{MAE}(H(x_i))^2  \geq
\text{MSE}(H(x_i))</span>, since summing up the errors before squaring
them is greater than the other way around!</p>
<!-- This equation looks very similar to the mean squared error! We can actually take the square root of each sides of this equation to learn more about which side is larger:
$$
\begin{align*}
(\text{MAE}(H^\circ))^2 &\_\_\_ \text{MSE}(H^\circ) \\
\sqrt{(\text{MAE}(H^\circ))^2} &\_\_\_ \sqrt{\text{MSE}(H^\circ)} \\
\text{MAE}(H^\circ) &\_\_\_ \sqrt{\text{MSE}(H^\circ)}
\end{align*}
$$

The square root of $\text{MSE}$ is equal to $\text{MAE}$! These two elements are basically the same because squaring a value will lead to non-negatives and then it will be square rooted to match the absolute value from $\text{MAE}$. However the $\text{MSE}$ being square rooted also allows for it to be smaller than the $\text{MSE}$ making the correct symbol $\geq$. -->
</div>
</div>
</div>
</div>
<p><br></p>
<hr />
<h2 id="problem-4">Problem 4</h2>
<p><i>Source: <a href="../wi22-midterm1/index.html">Winter 2022 Midterm
1</a>, Problem 4</i></p>
<p>Consider the dataset shown below.</p>
<div class="center">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span
class="math inline">x^{(1)}</span></th>
<th style="text-align: left;"><span
class="math inline">x^{(2)}</span></th>
<th style="text-align: left;"><span
class="math inline">x^{(3)}</span></th>
<th style="text-align: left;"><span class="math inline">y</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">-5</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5</td>
<td style="text-align: left;">-1</td>
<td style="text-align: left;">-3</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">0</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">2</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<h3 id="problem-4.1">Problem 4.1</h3>
<p>We want to use multiple regression to fit a prediction rule of the
form <span class="math display">H(x^{(1)}, x^{(2)}, x^{(3)}) = w_0 + w_1
x^{(1)}x^{(3)} + w_2 (x^{(2)}-x^{(3)})^2.</span> Write down the design
matrix <span class="math inline">X</span> and observation vector <span
class="math inline">\vec{y}</span> for this scenario. No justification
needed.</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading4_1">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse4_1" aria-expanded="true" aria-controls="collapse4_1">
Click to view the solution.
</button>
</h2>
<div id="collapse4_1" class="accordion-collapse collapse"
aria-labelledby="heading4_1" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p>The design matrix <span class="math inline">X</span> and observation
vector <span class="math inline">\vec{y}</span> are given by</p>
<p><span class="math display">
\begin{align*}
X &amp;=
\begin{bmatrix}
1 &amp; 0 &amp; 4\\
1 &amp; 15 &amp; 1\\
1 &amp; -15 &amp; 4\\
1 &amp; 0 &amp; 1
\end{bmatrix} \\
&amp;\text{and} \\
\vec{y} &amp;= \begin{bmatrix}
-5\\
7\\
4\\
2
\end{bmatrix}
\end{align*}
</span></p>
<p>We got <span class="math inline">\vec{y}</span> by looking at our
dataset and seeing the <span class="math inline">y</span> column.</p>
<p>The matrix <span class="math inline">X</span> was found by looking at
the equation <span class="math inline">H(x)</span>. You can think of
each row of <span class="math inline">X</span> being: <span class="math inline">\begin{bmatrix}1 &amp; x^{(1)}x^{(3)} &amp;
(x^{(2)}-x^{(3)})^2\end{bmatrix}</span>. Recall our bias term here is
not affected by <span class="math inline">x^{(i)}</span>, but it still
exists! So we will always have the first element in our row be <span class="math inline">1</span>. We can then easily calculate the other
elements in the matrix.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-4.2">Problem 4.2</h3>
<p>For the <span class="math inline">X</span> and <span
class="math inline">\vec{y}</span> that you have written down, let <span
class="math inline">\vec{w}</span> be the optimal parameter vector,
which comes from solving the normal equations <span
class="math inline">X^TX\vec{w}=X^T\vec{y}</span>. Let <span
class="math inline">\vec{e} = \vec{y} - X \vec{w}</span> be the error
vector, and let <span class="math inline">e_i</span> be the <span
class="math inline">i</span>th component of this error vector. Show that
<span class="math display">4e_1+e_2+4e_3+e_4=0.</span></p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading4_2">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse4_2" aria-expanded="true" aria-controls="collapse4_2">
Click to view the solution.
</button>
</h2>
<div id="collapse4_2" class="accordion-collapse collapse"
aria-labelledby="heading4_2" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p>The key to this problem is the fact that the error vector, <span class="math inline">\vec{e}</span>, is orthogonal to the columns of the
design matrix, <span class="math inline">X</span>. As a refresher, if
<span class="math inline">\vec{w^*}</span> satisfies the normal
equations, then:</p>
<p>We can rewrite the normal equation (<span class="math inline">X^TX\vec{w}=X^T\vec{y}</span>) to allow substitution
for <span class="math inline">\vec{e} = \vec{y} - X \vec{w}</span>.</p>
<p><span class="math display">
\begin{align*}
X^TX\vec{w}&amp;=X^T\vec{y} \\
0 &amp;= X^T\vec{y} - X^TX\vec{w} \\
0 &amp;= X^T(\vec{y}-X\vec{w}) \\
0 &amp;= X^T\vec{e}
\end{align*}
</span></p>
<p>The first step is to find <span class="math inline">X^T</span>, which
is easy because we found <span class="math inline">X</span> above: <span class="math display">
\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 15 &amp; -15 &amp; 0 \\ 4 &amp; 1
&amp; 4 &amp; 1
\end{bmatrix}
</span></p>
<p>And now we can plug <span class="math inline">X^T</span> and <span class="math inline">\vec e</span> into our equation <span class="math inline">0 = X^T\vec{e}</span>. It might be easiest to find
the right side first: <span class="math display">
\begin{align*}
X^T\vec{e} &amp;=
\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 15 &amp; -15 &amp; 0 \\ 4 &amp; 1
&amp; 4 &amp; 1
\end{bmatrix} \cdot \begin{bmatrix} e_1 \\ e_2 \\ e_3 \\
e_4\end{bmatrix} \\
&amp;= \begin{bmatrix} e_1 + e_2 + e_3 + e_4 \\
15e_2 - 15e_3 \\ 4e_1 + e_2 + 4e_3 + e_4\end{bmatrix}
\end{align*}
</span></p>
<p>Finally, we set it equal to zero! <span class="math display">
\begin{align*}
0 &amp;= e_1 + e_2 + e_3 + e_4 \\
0 &amp;= 15e_2 - 15e_3 \\
0 &amp;= 4e_1 + e_2 + 4e_3 + e_4
\end{align*}
</span></p>
<p>With this we have shown that <span class="math inline">4e_1+e_2+4e_3+e_4=0</span>.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<hr />
<h2 id="problem-5">Problem 5</h2>
<p><i>Source: <a href="../wi24-midterm1/index.html">Winter 2024 Midterm
1</a>, Problem 5</i></p>
<p>Suppose the following information is given for linear regression:</p>
<p><span class="math inline">X = \begin{bmatrix}
1 &amp; 2\\
1 &amp; -1
\end{bmatrix}</span></p>
<p><span class="math inline">\vec{y} =
    \begin{bmatrix}
    a\\
    b\\
    \end{bmatrix}</span> <span class="math inline">\vec{w}^{*} =
    \begin{bmatrix}
    1\\
    2\\
    \end{bmatrix}</span></p>
<p>Where <span class="math inline">X</span> is the design matrix, <span
class="math inline">\vec{y}</span> is the observation vector, and <span
class="math inline">\vec{w}^{*}</span> is the optimal parameter vector.
Solve for parameters <span class="math inline">a</span> and <span
class="math inline">b</span> using the normal equations, show your
work.</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading5">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse5" aria-expanded="true" aria-controls="collapse5">
Click to view the solution.
</button>
</h2>
<div id="collapse5" class="accordion-collapse collapse"
aria-labelledby="heading5" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math display">\begin{cases}
       a = 5\\
       b = -1\\
\end{cases}</span></p>
<p>Since <span class="math inline">\vec{w}^{*}</span> is the optimal
parameter vector, it must satisfy the normal equations:</p>
<p><span class="math display">\begin{align*}
        X^{T}X\vec{w} = X^{T}\vec{y}
\end{align*}</span></p>
<p>The left hand side of the equation will read:</p>
<p><span class="math display">\begin{align*}
X^{T}X\vec{w} &amp;=
\begin{bmatrix}
1 &amp; 1\\
2 &amp; -1
\end{bmatrix}

\begin{bmatrix}
1 &amp; 2\\
1 &amp; -1
\end{bmatrix}

\begin{bmatrix}
1\\
2
\end{bmatrix}
\\
&amp;=


\begin{bmatrix}
2 &amp; 1\\
1 &amp; 5
\end{bmatrix}

\begin{bmatrix}
1\\
2
\end{bmatrix}
\\
&amp;=

\begin{bmatrix}
4\\
11
\end{bmatrix}
\end{align*}</span></p>
<p>The right hand side of the equation is given by:</p>
<p><span class="math display">\begin{align*}
X^{T}\vec{y} &amp;=
\begin{bmatrix}
1 &amp; 1\\
2 &amp; -1
\end{bmatrix}
\begin{bmatrix}
a\\
b
\end{bmatrix}

\\
&amp;=

\begin{bmatrix}
a+b\\
2a-b
\end{bmatrix}
\end{align*}</span></p>
<p>By setting the left hand side and right hand side equal to each
other, we will obtain the following system of equations:</p>
<p><span class="math display">\begin{align*}
\begin{bmatrix}
4\\
11
\end{bmatrix}

=

\begin{bmatrix}
a+b\\
2a-b
\end{bmatrix}
\end{align*}</span></p>
<p><span class="math display">\begin{cases}
        &amp;4 = a + b\\
        &amp;11 = 2a-b
\end{cases}</span></p>
<p>To solve this equation set, we can add them together: <span class="math display">\begin{align*}
       4+11 &amp;= a + b + 2a -b\\
       3a &amp;= 15\\
       \\
       \\
\end{align*}</span></p>
<p><span class="math display">\begin{cases}
       a = 5\\
       b = -1\\
\end{cases}</span></p>
</div>
</div>
</div>
</div>
<hr />
<h2 id="problem-6">Problem 6</h2>
<p><i>Source: <a href="../wi24-final-pt1/index.html">Winter 2024 Final
Part 1</a>, Problem 4</i></p>
<p>Albert collected 400 data points from a radiation detector. Each data
point contains 3 features: feature <span class="math inline">A</span>,
feature <span class="math inline">B</span> and feature <span
class="math inline">C</span>. The true particle energy <span
class="math inline">E</span> is also reported. Albert wants to design a
linear regression algorithm to predict the energy <span
class="math inline">E</span> of each particle, given a combination of
one or more of feature <span class="math inline">A</span>, <span
class="math inline">B</span>, and <span class="math inline">C</span>. As
the first step, Albert calculated the correlation coefficients among
<span class="math inline">A</span>, <span class="math inline">B</span>,
<span class="math inline">C</span> and <span
class="math inline">E</span>. He wrote it down in the following table,
where each cell of the table represents the correlaton of two terms:</p>
<div>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">A</span>   </th>
<th><span class="math inline">B</span>   </th>
<th><span class="math inline">C</span>   </th>
<th><span class="math inline">E</span>   </th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">A</span></td>
<td>1</td>
<td>-0.99</td>
<td>0.13</td>
<td>0.8</td>
</tr>
<tr class="even">
<td><span class="math inline">B</span></td>
<td>-0.99</td>
<td>1</td>
<td>0.25</td>
<td>-0.95</td>
</tr>
<tr class="odd">
<td><span class="math inline">C</span></td>
<td>0.13</td>
<td>0.25</td>
<td>1</td>
<td>0.72</td>
</tr>
<tr class="even">
<td><span class="math inline">E</span></td>
<td>0.8</td>
<td>-0.95</td>
<td>0.72</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<h3 id="problem-6.1">Problem 6.1</h3>
<p>Albert wants to start with a simple model: fitting only a single
feature to obtain the true energy (i.e. <span class="math inline">y =
w_0+w_1 x</span>). Which feature should he choose as <span
class="math inline">x</span> to get the lowest mean square error?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">A</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">B</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">C</span></p></li>
</ul>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading6_1">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse6_1" aria-expanded="true" aria-controls="collapse6_1">
Click to view the solution.
</button>
</h2>
<div id="collapse6_1" class="accordion-collapse collapse"
aria-labelledby="heading6_1" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math inline">B</span></p>
<p><span class="math inline">B</span> is the correct answer, because it
has the highest absolute correlation (0.95), the negative sign in front
of <span class="math inline">B</span> just means it is negatively
correlated to energy, and it can be compensated by a negative sign in
the weight.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-6.2">Problem 6.2</h3>
<p>Albert wants to add another feature to his linear regression in part
(a) to further boost the model’s performance. (i.e. <span
class="math inline">y = w_0 + w_1 x + w_2 x_2</span>) Which feature
should he choose as <span class="math inline">x_2</span> to make
additional improvements?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">A</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">B</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">C</span></p></li>
</ul>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading6_2">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse6_2" aria-expanded="true" aria-controls="collapse6_2">
Click to view the solution.
</button>
</h2>
<div id="collapse6_2" class="accordion-collapse collapse"
aria-labelledby="heading6_2" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math inline">C</span></p>
<p><span class="math inline">C</span> is the correct answer, because
although <span class="math inline">A</span> has a higher correlation
with energy, it also has an extremely high correlation with <span class="math inline">B</span> (-0.99), that means adding <span class="math inline">A</span> into the fit will not be too useful, since
it provides almost the same information as <span class="math inline">B</span>.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-6.3">Problem 6.3</h3>
<p>Albert further refines his algorithm by fitting a prediction rule of
the form: <span class="math display">\begin{aligned}
H(A,B,C) = w_0 + w_1 \cdot A\cdot C + w_2 \cdot B^{C-7}
\end{aligned}</span></p>
<p>Given this prediction rule, what are the dimensions of the design
matrix <span class="math inline">X</span>?</p>
<p><span class="math display">\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
\end{bmatrix}_{r \times c}</span></p>
<p>So, what are <span class="math inline">r</span> and <span
class="math inline">c</span> in <span class="math inline">r \text{ rows}
\times c \text{ columns}</span>?</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading6_3">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse6_3" aria-expanded="true" aria-controls="collapse6_3">
Click to view the solution.
</button>
</h2>
<div id="collapse6_3" class="accordion-collapse collapse"
aria-labelledby="heading6_3" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math inline">400 \text{ rows} \times 3 \text{
columns}</span></p>
<p>Recall there are 400 data points, which means there will be 400 rows.
There will be 3 columns; one is the bias column of all <span class="math inline">1</span>s, one is for the feature <span class="math inline">A\cdot C</span>, and one is for the feature <span class="math inline">B^{C-7}</span>.</p>
</div>
</div>
</div>
</div>
<p><br></p>
<!-- Commented out the last subproblem, because the solution is weird?? Need to rework. -->
<!-- <br>

### Problem 6.4



Now Albert solves the normal equations and finds the solution to be:
$$\vec{w}^* = \begin{bmatrix} w_0^* \\ w_1^* \\ w_2^*  \end{bmatrix}$$
To improve on this result, Albert decides to modify his design matrix
with the following steps:

1.  Add 1 to every entry to the first column

2.  Swap the second and the third column

Let $X_a$ be the modified design matrix. Let
$\vec{w_a}^* = (X_a^TX_a)^{-1}X_a^T\vec{y}$. 

Express the components
$\vec{w_a}^*$ in terms of $w_0^*, w_1^*, w_2^*$, which were the
components of $\vec{w}^*$.

$$\vec{w_a}^* = \begin{bmatrix} ? \\ ? \\ ? \\ \end{bmatrix}$$






<div class="accordion" id="accordionExample">
<div class="accordion-item">
<h2 class="accordion-header" id="heading6_4">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse6_4" aria-expanded="true" aria-controls="collapse6_4">
Click to view the solution.
</button>
</h2>
<div id="collapse6_4" class="accordion-collapse collapse collapse" aria-labelledby="heading6_4" data-bs-parent="#accordionExample">
<div class="accordion-body">

<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math display">\vec{w}^* = \begin{bmatrix} w_0^*/2 \\
w_2^* \\ w_1^*  \end{bmatrix}</span></p>
<p>We should follow the steps Albert did to construct <span class="math inline">X</span>. We will add <span class="math inline">1</span> to be the first element in each row and
then switch the constants attached to <span class="math inline">w_1</span> (<span class="math inline">A \cdot
B</span>) and <span class="math inline">w_2</span> (<span class="math inline">B^{C-7}</span>): <span class="math display">
X_a = \begin{bmatrix}
1 &amp; B^{C-7} &amp; A \\
1 &amp; B^{C-7} &amp; A \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; B^{C-7} &amp; A
\end{bmatrix}
</span></p>
<p>We know that <span class="math inline">X_\alpha^T X_\alpha</span> is
the same as an identity matrix. The inverse of an identity matrix is the
same as it was before.</p>
<p>This means we are really looking at <span class="math inline">X_\alpha^T \vec{y}</span>: <span class="math display">
\begin{align*}
X_\alpha^T \vec{y} &amp;= \begin{bmatrix}
1 &amp; 1 &amp; \cdots &amp; 1 \\
B^{C-7} &amp; B^{C-7} &amp; \cdots &amp; B^{C-7} \\
A &amp; A &amp; \cdots &amp; A
\end{bmatrix} \cdot \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_{400}
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\sum_{i = 1}^{400}{y_i}\\
\sum_{i = 1}^{400}{B^{C-7} \cdot y_i} \\
\sum_{i = 1}^{400}{A \cdot y_i}
\end{bmatrix}
\end{align*}
</span></p>
<p>The first component of our final matrix, <span class="math inline">sum_{i = 1}^{400}{y_i}</span>, is proportional to
<span class="math inline">w_0^*</span>. The second component of our
matrix, <span class="math inline">\sum_{i = 1}^{400}{B^{C-7} \cdot
y_i}</span>, is proportional to <span class="math inline">w_2^*</span>.
Finally, the third component of our matrix, <span class="math inline">\sum_{i = 1}^{400}{A \cdot y_i}</span>, is
proportional to <span class="math inline">w_1^*</span>.</p>

</div>
</div>
</div>
</div>




    
<br> -->
<hr />
<h2 id="problem-7">Problem 7</h2>
<p><i>Source: <a href="../fa21-final/index.html">Fall 2021 Final
Exam</a>, Problem 6</i></p>
<p>Billy’s aunt owns a jewellery store, and gives him data on <span
class="math inline">5000</span> of the diamonds in her store. For each
diamond, we have:</p>
<ul>
<li><strong>carat</strong>: the weight of the diamond, in carats</li>
<li><strong>length</strong>: the length of the diamond, in
centimeters</li>
<li><strong>width</strong>: the width of the diamond, in
centimeters</li>
<li><strong>price</strong>: the value of the diamond, in dollars</li>
</ul>
<p>The first 5 rows of the 5000-row dataset are shown below:</p>
<div>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 27%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="header">
<th>carat   </th>
<th>length   </th>
<th>width   </th>
<th>price   </th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.40</td>
<td>4.81</td>
<td>4.76</td>
<td>1323</td>
</tr>
<tr class="even">
<td>1.04</td>
<td>6.58</td>
<td>6.53</td>
<td>5102</td>
</tr>
<tr class="odd">
<td>0.40</td>
<td>4.74</td>
<td>4.76</td>
<td>696</td>
</tr>
<tr class="even">
<td>0.40</td>
<td>4.67</td>
<td>4.65</td>
<td>798</td>
</tr>
<tr class="odd">
<td>0.50</td>
<td>4.90</td>
<td>4.95</td>
<td>987</td>
</tr>
</tbody>
</table>
</div>
<p><br> Billy has enlisted our help in predicting the price of a diamond
given various other features.</p>
<p><br></p>
<h3 id="problem-7.1">Problem 7.1</h3>
<p>Suppose we want to fit a linear prediction rule that uses two
features, carat and length, to predict price. Specifically, our
prediction rule will be of the form</p>
<p><span class="math display">\text{predicted price} = w_0 + w_1 \cdot
\text{carat} + w_2 \cdot \text{length}</span></p>
<p><br></p>
<p>We will use least squares to find <span class="math inline">\vec{w}^*
= \begin{bmatrix} w_0^* \\ w_1^* \\ w_2^* \end{bmatrix}</span>.</p>
<p>Write out the first 5 rows of the design matrix, <span
class="math inline">X</span>. Your matrix should not have any variables
in it.</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading7_1">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse7_1" aria-expanded="true" aria-controls="collapse7_1">
Click to view the solution.
</button>
</h2>
<div id="collapse7_1" class="accordion-collapse collapse"
aria-labelledby="heading7_1" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p><span class="math display">X = \begin{bmatrix} 1 &amp; 0.40 &amp;
4.81 \\ 1 &amp; 1.04 &amp; 6.58 \\ 1 &amp; 0.40 &amp; 4.74 \\ 1 &amp;
0.40 &amp; 4.67 \\ 1 &amp; 0.50 &amp; 4.90 \end{bmatrix}</span></p>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-7.2">Problem 7.2</h3>
<p>Suppose the optimal parameter vector <span
class="math inline">\vec{w}^*</span> is given by</p>
<p><span class="math display">\vec{w}^* = \begin{bmatrix} 2000 \\ 10000
\\ -1000 \end{bmatrix}</span></p>
<p>What is the predicted price of a diamond with 0.65 carats and a
length of 4 centimeters? Show your work.</p>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading7_2">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse7_2" aria-expanded="true" aria-controls="collapse7_2">
Click to view the solution.
</button>
</h2>
<div id="collapse7_2" class="accordion-collapse collapse"
aria-labelledby="heading7_2" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<p>The predicted price is</p>
<p><span class="math display">2000 + 10000 \cdot 0.65 - 1000 \cdot 4 =
4500</span></p>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-7.3">Problem 7.3</h3>
<p>Suppose <span class="math inline">\vec{e} = \begin{bmatrix} e_1 \\
e_2 \\ ... \\ e_n \end{bmatrix}</span> is the error/residual vector,
defined as</p>
<p><span class="math display">\vec{e} = \vec{y} - X \vec{w}^*</span></p>
<p>where <span class="math inline">\vec{y}</span> is the observation
vector containing the prices for each diamond.</p>
<p>For each of the following quantities, state whether they are
guaranteed to be equal to 0 the scalar, <span
class="math inline">\vec{0}</span> the vector of all 0s, or neither. No
justification is necessary.</p>
<ul>
<li><span class="math inline">\sum_{i = 1}^n e_i</span></li>
<li><span class="math inline">|| \vec{y} - X \vec{w}^* ||^2</span></li>
<li><span class="math inline">X^TX \vec{w}^*</span></li>
<li><span class="math inline">2X^TX \vec{w}^* - 2X^T\vec{y}</span></li>
</ul>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading7_3">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse7_3" aria-expanded="true" aria-controls="collapse7_3">
Click to view the solution.
</button>
</h2>
<div id="collapse7_3" class="accordion-collapse collapse"
aria-labelledby="heading7_3" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<ul>
<li><span class="math inline">\sum_{i = 1}^n e_i</span>: Yes, this is
guaranteed to be 0. This was discussed in Homework 4; it is a
consequence of the fact that <span class="math inline">X^T (y - X
\vec{w}^*) = 0</span> and that we have an intercept term in our
prediction rule (and hence a column of all 1s in our design matrix,
<span class="math inline">X</span>).</li>
<li><span class="math inline">|| \vec{y} - X \vec{w}^* ||^2</span>: No,
this is not guaranteed to be 0. This is the mean squared error of our
prediction rule, multiplied by <span class="math inline">n</span>. <span class="math inline">\vec{w}^*</span> is found by minimizing mean squared
error, but the minimum value of mean squared error isn’t necessarily 0 —
in fact, this quantity is only 0 if we can write <span class="math inline">\vec{y}</span> exactly as <span class="math inline">X \vec{w}^*</span> with no prediction errors.</li>
<li><span class="math inline">X^TX \vec{w}^*</span>: This is not
guaranteed to be 0, either.</li>
<li><span class="math inline">2X^TX \vec{w}^* - 2X^T\vec{y}</span>: This
is guaranteed to be 0. Recall, the optimal parameter vector <span class="math inline">\vec{w}^*</span> satisfies the normal equations
<span class="math inline">X^TX\vec{w}^* = X^T \vec{y}</span>.
Subtracting <span class="math inline">X^T \vec{y}</span> from both sides
of this equation and multiplying both sides by 2 yields the desired
result. (You may also find <span class="math inline">2X^TX \vec{w}^* -
2X^T\vec{y}</span> to be familiar from lecture — it is the gradient of
the mean squared error, multiplied by <span class="math inline">n</span>, and we set the gradient to 0 to find <span class="math inline">\vec{w}^*</span>.)</li>
</ul>
</div>
</div>
</div>
</div>
<p><br></p>
<h3 id="problem-7.4">Problem 7.4</h3>
<p>Suppose we introduce two more features:</p>
<ul>
<li>width alone, and</li>
<li>area, which is defined as length times width</li>
</ul>
<p>Suppose we also decide to remove the intercept term of our prediction
rule. With all of these changes, our prediction rule is now</p>
<p><span class="math display">\text{predicted price} = w_1 \cdot
\text{carat} + w_2 \cdot \text{length} + w_3 \cdot \text{width} + w_4
\cdot (\text{length} \cdot \text{width}) </span></p>
<ul>
<li>Write out just the first 2 rows of the design matrix <span
class="math inline">X</span> for this new prediction rule. You do
<strong>not</strong> need to simplify the numbers in your matrix, it is
fine if they involve the multiplication symbol.</li>
<li>Is the optimal coefficient for carat, <span
class="math inline">w_1^*</span>, for this new prediction rule
guaranteed to be equal to 10000, the optimal coefficient for carat in
our original prediction rule? No justification is necessary.</li>
</ul>
<div id="accordionExample" class="accordion">
<div class="accordion-item">
<h2 class="accordion-header" id="heading7_4">
<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse7_4" aria-expanded="true" aria-controls="collapse7_4">
Click to view the solution.
</button>
</h2>
<div id="collapse7_4" class="accordion-collapse collapse"
aria-labelledby="heading7_4" data-bs-parent="#accordionExample">
<div class="accordion-body">
<header id="title-block-header">
<h1 class="title"> </h1>
</header>
<ul>
<li><span class="math inline">X = \begin{bmatrix} 0.40 &amp; 4.81 &amp;
4.76 &amp; 4.81 \cdot 4.76 \\ 1.04 &amp; 6.58 &amp; 6.53 &amp; 6.58
\cdot 6.53 \end{bmatrix}</span></li>
<li>No, it’s not guaranteed that the <span class="math inline">\vec{w}_1^*</span> for this new prediction rule is
equal to the <span class="math inline">\vec{w}_1^*</span> for the
original prediction rule. The value of <span class="math inline">\vec{w}_1^*</span> in the new prediction rule will
be influenced by the fact that there’s no longer an intercept term and
that there are two new features (width and area) that weren’t previously
there.</li>
</ul>
</div>
</div>
</div>
</div>
<p><br></p>
<hr />
<h2 id="section"><span class="math display"> </span></h2>
<h4
id="feedback-find-an-error-still-confused-have-a-suggestion-let-us-know-here.">👋
Feedback: Find an error? Still confused? Have a suggestion?
<a href="https://forms.gle/WZ71FchnXU1K154d7">Let us know
here</u></a>.</h4>
<hr />
</body>
</html>
