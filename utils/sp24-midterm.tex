
\documentclass[twoside,12pt]{article}

\usepackage{dsctemplate}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{fancyhdr}
\usepackage{nicefrac}
\usepackage{minted}
\usetikzlibrary{quotes,angles,positioning,arrows.meta}
\usetikzlibrary{calc}
\usepackage{enumitem}
\usepackage{fancyvrb}
\usepackage{dirtytalk}
\usepackage{comment}
\usepackage{graphicx}

\DefineVerbatimEnvironment{verbatim}{Verbatim}{xleftmargin=.5in}
\newlist{multiplechoice}{itemize}{2}
\setlist[multiplechoice]{label=$\square$}

% configuration
% ------------------------------------------------------------------------------

% control whether solutions are shown or hidden
% \showsolntrue

% page headers only on odd pages
\pagestyle{fancy}
\fancyhead{}
\fancyhead[RO]{PID: \rule{3in}{.5pt}}
\renewcommand{\headrulewidth}{0pt}
% \pagenumbering{gobble}

% ------------------------------------------------------------------------------

\begin{document}


\thispagestyle{empty}

\vspace{-5.5in}

\pstitle{%
    Midterm Exam
}{DSC 40A, Spring 2024}

\vspace{-.3in}

\begin{tabular}{rl}
    Full Name: & \inlineresponsebox[4in]{Solutions}\\
    PID: & \inlineresponsebox[4in]{A12345678}\\
    Seat Number: & \inlineresponsebox[4in]{A1}
    % Lecture: & ( ) A (10AM)} ( ) B (9AM)} \bubble{C (1PM)} \bubble{D (8AM) \vspace{.3in \\ 
\end{tabular}

\vspace{.1in}

\hline

\vspace{.1in}


\textbf{Instructions:}
    \begin{itemize}
        \item This exam consists of 5 questions, worth a total of 64 points. \textbf{Advice: Read all of the questions before starting to work, because the questions are not sorted by difficulty.}
        \item Write your PID in the top right corner of each page in the space provided.
        \item Please write \textbf{clearly} in the provided answer boxes; we will not grade work that appears elsewhere.
        \begin{itemize}
            \item For questions that ask you to show your work, correct answers with no work shown will receive no credit.
            \item When asked to do so, please place your final answer in a $\boxed{\text{box}}$.
            \item ( ) In multiple choice questions, select only one option and completely fill in the corresponding bubble --- if we cannot tell which option you selected, you may not receive credit.
        \end{itemize}
        \item You may refer to a single two-sided index card of maximum size 4 inches by 6 inches that you wrote on by hand by yourself. Other than that, you may not refer to any resources or technology during the exam (no phones, no smart watches, no computers, and no calculators).
    \end{itemize}

\vspace{.1in}

\hline

\vspace{.2in}

\noindent By signing below, you are agreeing that you will behave honestly and fairly during
and after this exam. 

\begin{tabular}{rl}
    \: \: \: \: \: Signature: & \inlineresponsebox[4in]{}\\
\end{tabular}

\vfill

\begin{center}
{\huge Version A} \vspace{.2in}

Please do not open your exam until instructed to do so.

\end{center}

\newpage



# BEGIN PROB

[(14 pts)]

Vectors get lonely, and so we will give each vector one friend to keep them company.

Specifically, if $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}$, $\vec{v_f}$ is the friend of $\vec{v}$, where $\vec{v_f} = \begin{bmatrix} -v_2 \\ v_1 \end{bmatrix}$.



# BEGIN SUBPROB

 Prove that $\vec{v}$ and $\vec{v_f}$ are orthogonal.

\begin{responsebox}{1in}
    
\end{responsebox}
    


# END SUBPROB



Consider the vectors $\vec{c}$ and $\vec{d}$, defined below.

$$\vec{c} = \begin{bmatrix} 1 \\ 7 \end{bmatrix} \qquad \vec{d} = \begin{bmatrix} -2 \\ 1 \end{bmatrix}$$

The next few parts ask you to write various vectors as scalar multiples of either $\vec{c}$, $\vec{c_f}$, $\vec{d}$, or $\vec{d_f}$, where $\vec{c_f}$ and $\vec{d_f}$ are the friends of $\vec{c}$ and $\vec{d}$, respectively. In each part, write \textbf{one number} in the box, and fill in \textbf{one bubble}. Part (b) has already been done for you.



# BEGIN SUBPROB

A vector in $\text{span}(\vec{d})$ that is twice as long as $\vec{d}$.

\tikz[baseline=-.5em]{
    \node[
        draw, rectangle, inner sep=0, text centered, minimum height=3em, 
        text width=1.5in, align=center
    ] at (0,0) {
    $2$
    };
    \useasboundingbox 
        ([shift={(1mm,1mm)}]current bounding box.north east)
        rectangle 
        ([shift={(-1mm,-1mm)}]current bounding box.south west);
} $\times$ ( ) $\vec{c}$} ( ) $\vec{c_f}$} \bubble[fill=black!90]{$\vec{d}$} \bubble{$\vec{d_f$



# END SUBPROB

# BEGIN SUBPROB

 The projection of $\vec{c}$ onto $\text{span}(\vec{d})$.

\inlineresponsebox[1.5in]{1in} $\times$ ( ) $\vec{c}$} ( ) $\vec{c_f}$} \bubble{$\vec{d}$} \bubble{$\vec{d_f$



# END SUBPROB

# BEGIN SUBPROB

 The error vector of the projection of $\vec{c}$ onto $\text{span}(\vec{d})$.

\inlineresponsebox[1.5in]{1in} $\times$ ( ) $\vec{c}$} ( ) $\vec{c_f}$} \bubble{$\vec{d}$} \bubble{$\vec{d_f$



# END SUBPROB

# BEGIN SUBPROB

 The projection of $\vec{d}$ onto $\text{span}(\vec{c})$.

\inlineresponsebox[1.5in]{1in} $\times$ ( ) $\vec{c}$} ( ) $\vec{c_f}$} \bubble{$\vec{d}$} \bubble{$\vec{d_f$



# END SUBPROB

# BEGIN SUBPROB

 The error vector of the projection of $\vec{d}$ onto $\text{span}(\vec{c})$.

\inlineresponsebox[1.5in]{1in} $\times$ ( ) $\vec{c}$} ( ) $\vec{c_f}$} \bubble{$\vec{d}$} \bubble{$\vec{d_f$



# END SUBPROB
    

    


# END PROB

\newpage

# BEGIN PROB

[(11 pts)]

Consider a dataset of $n$ values, $y_1, y_2, ..., y_n$, all of which are non-negative. We're interested in fitting a constant model, $H(x) = h$, to the data, using the new ``Sun God" loss function:

$$L_\text{sungod}(y_i, h) = w_i \left( y_i^2 - h^2  \right)^2$$

Here, $w_i$ corresponds to the ``weight" assigned to the data point $y_i$, the idea being that different data points can be weighted differently when finding the optimal constant prediction, $h^*$. 

\vspace{0.1in}

For example, for the dataset $y_1 = 1, y_2 = 5, y_3 = 2$, we will end up with different values of $h^*$ when we use the weights $w_1 = w_2 = w_3 = 1$ and when we use weights $w_1 = 8, w_2 = 4, w_3 = 3$.



# BEGIN SUBPROB

 Find $\frac{\partial L_\text{sungod}}{\partial h}$, the derivative of the Sun God loss function with respect to $h$. Show your work, and put a $\boxed{\text{box}}$ around your final answer.

\begin{responsebox}{3in}

\end{responsebox}
    


# END SUBPROB

\newpage

# BEGIN SUBPROB

 Prove that the constant prediction that minimizes empirical risk for the Sun God loss function is:

$$h^* = \sqrt{\frac{\sum_{i = 1}^n w_i y_i^2}{\sum_{i = 1}^n w_i}}$$

% \textit{Just as an example, for the dataset $y_1 = 1, y_2 = 5, y_3 = 2$:
% \begin{itemize}
%     \item Using the weights $w_1 = w_2 = w_3 = 1$, $h^* = \sqrt{10}$.
%     \item Using the weights $w_1 = 8, w_2 = 4, w_3 = 3$, $h^* = \sqrt{8}$.
% \end{itemize}
% }

\begin{responsebox}{5in}

\end{responsebox}
    


# END SUBPROB

# BEGIN SUBPROB

 For a dataset of non-negative values $y_1, y_2, ..., y_n$ with weights $w_1, 1, ..., 1$, evaluate: $$\displaystyle \lim_{w_1 \rightarrow \infty} h^*$$

( ) The maximum of $y_1, y_2, ..., y_n$

( ) The mean of $y_1, y_2, ..., y_{n-1}$

( ) The mean of $y_2, y_3, ..., y_n$

( ) The mean of $y_2, y_3, ..., y_n$, multiplied by $\frac{n}{n-1}$

( ) $y_1$

( ) $y_n$

    


# END SUBPROB
    

    


# END PROB

\newpage

# BEGIN PROB

[(12 pts)]

Consider a dataset of $n$ values, $y_1, y_2, ..., y_n$, where $y_1 < y_2 < ... < y_n$. Let $R_\text{abs}(h)$ be the mean absolute error of a constant prediction $h$ on this dataset of $n$ values.

Suppose that we introduce a new value to the dataset, $\alpha$. Let $S_\text{abs}(h)$ be the mean absolute error of a constant prediction $h$ on this new dataset of $n + 1$ values.

We're given that:

\begin{itemize}
    \item $n > 5$.
    \item $\alpha$ is not equal to any of $y_1, y_2, ..., y_n$. 
    \item All values of $h$ between 7 and 9 minimize $S_\text{abs}(h)$.
    \item The slope of $S_\text{abs}(h)$ on the line segment immediately to the right of $\alpha$ is $\frac{5-n}{1 + n}$.
\end{itemize}



# BEGIN SUBPROB

 In the problem statement, we were told that ``all values between 7 and 9 minimize $S_\text{abs}(h)$." More specifically, what interval of values $h$ minimize $S_\text{abs}(h)$? 

( ) $7 < h < 9$} ( ) $7 \leq h < 9$} \bubble{$7 < h \leq 9$ \bubble{$7 \leq h \leq 9$

    


# END SUBPROB

# BEGIN SUBPROB

 Which value(s) minimize $R_\text{abs}(h)$? Give your answer(s) as integer(s) with no variables. Show your work, and put a $\boxed{\text{box}}$ around your final answer(s).

\textit{Hint: Don't start by trying to expand $\frac{1}{n} \sum_{i = 1}^n |y_i - h|$ --- instead, think about what removing $\alpha$ does.}

\begin{responsebox}{2.5in}
    
\end{responsebox}
    


# END SUBPROB

\newpage

# BEGIN SUBPROB

 What is the slope of $S_\text{abs}(h)$ on the line segment immediately to the left of $\alpha$? Give your answer in the form of an expression involving $n$. Show your work, and put a $\boxed{\text{box}}$ around your final answer.

\begin{responsebox}{5in}
    
\end{responsebox}
    


# END SUBPROB

% \newpage

% # BEGIN SUBPROB



% Suppose, just in part (d), that $\alpha = 2$ and that the minimum value of $S_\text{abs}(h)$ is 16. What is the value of $n$? Give your answer as an integer with no variables.

% \textit{Hint: Let $M$ be the minimum value of $R_\text{abs}(h)$. Start by trying to express 16, the minimum value of $S_\text{abs}(h)$, in terms of $M$ and $n$. You will need to use your answers from parts (a) and (b).}

% \begin{responsebox}{6in}
    
% \end{responsebox}
    
% 

# END SUBPROB
    

    


# END PROB

\newpage

# BEGIN PROB

[(19 pts)]

Suppose we want to fit a hypothesis function of the form:

$$H(x) = w_0 + w_1 x^2$$

Note that this is \textit{not} the simple linear regression hypothesis function, $H(x) = w_0 + w_1x$.

To do so, we will find the optimal parameter vector $\vec{w}^* = \begin{bmatrix} w_0^* \\ w_1^* \end{bmatrix}$ that satisfies the normal equations. The first 5 rows of our dataset are as follows, though note that our dataset has $n$ rows in total.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
$x$ & $y$ \\ \hline
2   & 4   \\ \hline
-1  & 4   \\ \hline
3   & 4  \\ \hline
-7  & 4   \\ \hline
3   & 4   \\ \hline
\end{tabular}
\end{table}

Suppose that $x_1, x_2, ..., x_n$ have a mean of $\bar{x} = 2$ and a variance of $\sigma_x^2 = 10$.



# BEGIN SUBPROB

 Write out the first 5 rows of the design matrix, $X$.

\begin{responsebox}{1.5in}
    
\end{responsebox}



# END SUBPROB

# BEGIN SUBPROB

 Suppose, just in part (b), that after solving the normal equations, we find $\vec{w}^* = \begin{bmatrix} 2 \\ -5 \end{bmatrix}$. What is the predicted $y$ value for the augmented feature vector $\text{Aug}(\vec{x}) =  \begin{bmatrix} 1 \\ 4 \end{bmatrix}$? Give your answer as an integer with no variables. Show your work, and put 

a $\boxed{\text{box}}$ around your final answer.

\begin{responsebox}{1in}
    
\end{responsebox}
    


# END SUBPROB

\newpage

# BEGIN SUBPROB

 Let $X_\text{tri} = 3 X$. Using the fact that $\sum_{i = 1}^n x_i^2 = n \sigma_x^2 + n \bar{x}^2$, determine the value of the bottom-left value in the matrix $X_\text{tri}^T X_\text{tri}$, i.e. the value in the second row and first column. Give your answer as an expression involving $n$. Show your work, and put a $\boxed{\text{box}}$ around your final answer.

\begin{responsebox}{3.5in}
    
\end{responsebox}
    


# END SUBPROB

# BEGIN SUBPROB

 Consider the following four hypothesis functions:

\begin{itemize}
    \item $H_1(x) = H(x) = w_0 + w_1 x^2$
    \item $H_2(x) = w_0$
    \item $H_3(x) = w_0 + w_1 x$
    \item $H_4(x) = w_0 + w_1x + w_2x^2$
\end{itemize}

Let $H_1^*$, $H_2^*$, $H_3^*$, and $H_4^*$ be the versions of all four hypothesis functions that are using optimal parameters. In the subparts below, fill in the blanks.

% mean squared error is being computed on the dataset of $n$ points used to find the optimal parameters.

\begin{enumerate}[label=(\roman*)]
    \item The mean squared error of $H_1^*$ is \_\_\_\_ the mean squared error of $H_2^*$.

    \begin{tabular}{lll}
    ( ) greater than} & ( ) greater than or equal to} & \bubble{equal to} \\ \bubble{less than} & \bubble{less than or equal to & \bubble{impossible to tell
    \end{tabular}

    \item The mean squared error of $H_1^*$ is \_\_\_\_ the mean squared error of $H_3^*$.

    \begin{tabular}{lll}
    ( ) greater than} & ( ) greater than or equal to} & \bubble{equal to} \\ \bubble{less than} & \bubble{less than or equal to & \bubble{impossible to tell
    \end{tabular}

    \item The mean squared error of $H_1^*$ is \_\_\_\_ the mean squared error of $H_4^*$.

    \begin{tabular}{lll}
    ( ) greater than} & ( ) greater than or equal to} & \bubble{equal to} \\ \bubble{less than} & \bubble{less than or equal to & \bubble{impossible to tell
    \end{tabular}

    \item In \_\_\_\_ of the hypothesis functions $H_1^*$,$ H_2^*$, $H_3^*$, and $H_4^*$, the sum of the residuals of the function's predictions is 0.

    ( ) none} ( ) 1} \bubble{2} \bubble{3 \bubble{all 4
\end{enumerate}
    


# END SUBPROB
    

    


# END PROB

\newpage

# BEGIN PROB



Consider a dataset of $n$ points, $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ where:

\begin{itemize}
    \item $x_1 < x_2 < ... < x_n$, and $x_1, x_2, ..., x_n$ have a variance of $\sigma_x^2 = 15$ and a range of 20 (the range of a collection of values is the difference between the largest and smallest value).
    \item $y_1 > y_2 > ... > y_n$, and $y_1, y_2, ..., y_n$ have a variance of $\sigma_y^2 = 8$ and a range of 6.
\end{itemize}

We fit two linear hypothesis functions using squared loss:

\begin{itemize}
\item One hypothesis function is fit with a ``swapped" version of the dataset, where $x_1$ and $x_n$ are swapped --- that is, it uses the dataset $(x_n, y_1), (x_2, y_2), ..., (x_{n-1}, y_{n-1}), (x_1, y_n)$. Note that only two of the points in this dataset are different than in the original dataset. We'll call the optimal slope and intercept of this hypothesis function $w_1^\text{swap}$ and $w_0^\text{swap}$, respectively.
\item Another hypothesis function is fit with the original dataset, 

$(x_1, y_1), (x_2, y_2), ..., (x_{n-1}, y_{n-1}), (x_n, y_n)$. We'll call the optimal slope and intercept of this hypothesis function $w_1^\text{orig}$ and $w_0^\text{orig}$, respectively. 
\end{itemize}

On the next page, in the space provided, prove that:

$$\displaystyle | w_1^\text{swap} - w_1^\text{orig} | = \frac{8}{n}$$

\textit{Hint: Approach this problem similarly to Problem 2 on Homework 3 (``Shout for Stroud"). Also, think about how you can express $\sum_{i = 1}^n (x_i - \bar{x})^2$ in terms of $n$ and $\sigma_x^2$.}


\vspace{3.7in}

Feel free to use the space here for scratch work, but we will only grade what appears in the box on the next page.

\newpage

Put your proof for Question 5 in the box below.

\begin{responsebox}{7in}
    
\end{responsebox}
    


# END PROB

\newpage

Make sure you've written your PID in the space provided in the top right corner of every page of this exam.

Feel free to draw us a picture about DSC 40A below.

And here's a free point!

\begin{responsebox}{7.5in}
    
\end{responsebox}



\end{document}
